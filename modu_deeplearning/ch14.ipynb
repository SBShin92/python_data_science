{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 34ms/step - loss: 0.3341 - accuracy: 0.8424 - val_loss: 0.2684 - val_accuracy: 0.9023\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2604 - accuracy: 0.9102 - val_loss: 0.2255 - val_accuracy: 0.9285\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2281 - accuracy: 0.9240 - val_loss: 0.2071 - val_accuracy: 0.9369\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2182 - accuracy: 0.9294 - val_loss: 0.2016 - val_accuracy: 0.9385\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2135 - accuracy: 0.9294 - val_loss: 0.1971 - val_accuracy: 0.9385\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2057 - accuracy: 0.9325 - val_loss: 0.1932 - val_accuracy: 0.9362\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1978 - accuracy: 0.9348 - val_loss: 0.1846 - val_accuracy: 0.9385\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1915 - accuracy: 0.9376 - val_loss: 0.1803 - val_accuracy: 0.9415\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1877 - accuracy: 0.9407 - val_loss: 0.1749 - val_accuracy: 0.9423\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1833 - accuracy: 0.9423 - val_loss: 0.1709 - val_accuracy: 0.9446\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1806 - accuracy: 0.9407 - val_loss: 0.1763 - val_accuracy: 0.9415\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1789 - accuracy: 0.9423 - val_loss: 0.1654 - val_accuracy: 0.9454\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1751 - accuracy: 0.9438 - val_loss: 0.1621 - val_accuracy: 0.9462\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1715 - accuracy: 0.9438 - val_loss: 0.1641 - val_accuracy: 0.9446\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1732 - accuracy: 0.9446 - val_loss: 0.1589 - val_accuracy: 0.9454\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1674 - accuracy: 0.9459 - val_loss: 0.1610 - val_accuracy: 0.9485\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1703 - accuracy: 0.9428 - val_loss: 0.1585 - val_accuracy: 0.9454\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1654 - accuracy: 0.9441 - val_loss: 0.1515 - val_accuracy: 0.9446\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1611 - accuracy: 0.9469 - val_loss: 0.1514 - val_accuracy: 0.9508\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1614 - accuracy: 0.9461 - val_loss: 0.1496 - val_accuracy: 0.9500\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1600 - accuracy: 0.9464 - val_loss: 0.1473 - val_accuracy: 0.9454\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1564 - accuracy: 0.9461 - val_loss: 0.1439 - val_accuracy: 0.9469\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1543 - accuracy: 0.9479 - val_loss: 0.1445 - val_accuracy: 0.9492\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1520 - accuracy: 0.9489 - val_loss: 0.1435 - val_accuracy: 0.9477\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1517 - accuracy: 0.9489 - val_loss: 0.1416 - val_accuracy: 0.9492\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1495 - accuracy: 0.9494 - val_loss: 0.1419 - val_accuracy: 0.9500\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1508 - accuracy: 0.9482 - val_loss: 0.1390 - val_accuracy: 0.9492\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1482 - accuracy: 0.9487 - val_loss: 0.1373 - val_accuracy: 0.9492\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1483 - accuracy: 0.9492 - val_loss: 0.1362 - val_accuracy: 0.9492\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1442 - accuracy: 0.9510 - val_loss: 0.1355 - val_accuracy: 0.9492\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1446 - accuracy: 0.9500 - val_loss: 0.1369 - val_accuracy: 0.9531\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1447 - accuracy: 0.9482 - val_loss: 0.1371 - val_accuracy: 0.9531\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1435 - accuracy: 0.9518 - val_loss: 0.1327 - val_accuracy: 0.9523\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1438 - accuracy: 0.9507 - val_loss: 0.1320 - val_accuracy: 0.9515\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1404 - accuracy: 0.9510 - val_loss: 0.1297 - val_accuracy: 0.9538\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1404 - accuracy: 0.9518 - val_loss: 0.1317 - val_accuracy: 0.9515\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1384 - accuracy: 0.9515 - val_loss: 0.1285 - val_accuracy: 0.9515\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.9525 - val_loss: 0.1283 - val_accuracy: 0.9523\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1356 - accuracy: 0.9536 - val_loss: 0.1271 - val_accuracy: 0.9554\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1350 - accuracy: 0.9528 - val_loss: 0.1255 - val_accuracy: 0.9546\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1340 - accuracy: 0.9530 - val_loss: 0.1251 - val_accuracy: 0.9538\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1338 - accuracy: 0.9536 - val_loss: 0.1278 - val_accuracy: 0.9523\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1332 - accuracy: 0.9548 - val_loss: 0.1231 - val_accuracy: 0.9554\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1314 - accuracy: 0.9543 - val_loss: 0.1239 - val_accuracy: 0.9538\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1309 - accuracy: 0.9538 - val_loss: 0.1215 - val_accuracy: 0.9562\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1297 - accuracy: 0.9530 - val_loss: 0.1208 - val_accuracy: 0.9562\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1295 - accuracy: 0.9538 - val_loss: 0.1221 - val_accuracy: 0.9538\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1297 - accuracy: 0.9554 - val_loss: 0.1200 - val_accuracy: 0.9562\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1284 - accuracy: 0.9564 - val_loss: 0.1185 - val_accuracy: 0.9569\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9541 - val_loss: 0.1182 - val_accuracy: 0.9569\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9469\n",
      "Test accuracy: 0.9469230771064758\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.7638.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.7638.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.7715.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.7854.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.8800.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.8931.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.9092.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.9262.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.9308.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9292.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9323.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9323.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9315.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9315.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9315.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9315.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9323.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9277.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9331.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9285.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9346.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9285.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9338.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9308.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9338.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9323.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9331.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9308.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9331.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9338.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9354.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9346.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9338.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9377.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9354.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9377.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9385.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9392.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9377.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9385.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9408.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9415.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9431.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9408.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9446.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9438.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9438.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9454.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9454.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9454.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9515\n",
      "Test accuracy: 0.9515384435653687\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157574</td>\n",
       "      <td>0.948678</td>\n",
       "      <td>0.172594</td>\n",
       "      <td>0.946154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.948678</td>\n",
       "      <td>0.172019</td>\n",
       "      <td>0.946154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.156193</td>\n",
       "      <td>0.948935</td>\n",
       "      <td>0.173187</td>\n",
       "      <td>0.944615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154211</td>\n",
       "      <td>0.949961</td>\n",
       "      <td>0.169072</td>\n",
       "      <td>0.946923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.152928</td>\n",
       "      <td>0.950475</td>\n",
       "      <td>0.168837</td>\n",
       "      <td>0.945385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.019634</td>\n",
       "      <td>0.994868</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>0.982308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.018732</td>\n",
       "      <td>0.995638</td>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.983846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>0.097216</td>\n",
       "      <td>0.982308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.017269</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>0.097050</td>\n",
       "      <td>0.982308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.995381</td>\n",
       "      <td>0.096493</td>\n",
       "      <td>0.981538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.157574  0.948678  0.172594      0.946154\n",
       "1     0.156266  0.948678  0.172019      0.946154\n",
       "2     0.156193  0.948935  0.173187      0.944615\n",
       "3     0.154211  0.949961  0.169072      0.946923\n",
       "4     0.152928  0.950475  0.168837      0.945385\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.019634  0.994868  0.099344      0.982308\n",
       "1996  0.018732  0.995638  0.090763      0.983846\n",
       "1997  0.017488  0.995894  0.097216      0.982308\n",
       "1998  0.017269  0.995894  0.097050      0.982308\n",
       "1999  0.017059  0.995381  0.096493      0.981538\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 33ms/step - loss: 1.1617 - accuracy: 0.5725 - val_loss: 0.2840 - val_accuracy: 0.8900\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3598 - accuracy: 0.8548 - val_loss: 0.3651 - val_accuracy: 0.8715\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4008 - accuracy: 0.8537 - val_loss: 0.3372 - val_accuracy: 0.8938\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3505 - accuracy: 0.8820 - val_loss: 0.2884 - val_accuracy: 0.9077\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3004 - accuracy: 0.9033 - val_loss: 0.2631 - val_accuracy: 0.9185\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2718 - accuracy: 0.9158 - val_loss: 0.2339 - val_accuracy: 0.9277\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2438 - accuracy: 0.9230 - val_loss: 0.2183 - val_accuracy: 0.9377\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2315 - accuracy: 0.9243 - val_loss: 0.2145 - val_accuracy: 0.9369\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2265 - accuracy: 0.9271 - val_loss: 0.2128 - val_accuracy: 0.9385\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2244 - accuracy: 0.9266 - val_loss: 0.2114 - val_accuracy: 0.9385\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2223 - accuracy: 0.9271 - val_loss: 0.2097 - val_accuracy: 0.9385\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2199 - accuracy: 0.9274 - val_loss: 0.2078 - val_accuracy: 0.9385\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2178 - accuracy: 0.9271 - val_loss: 0.2061 - val_accuracy: 0.9392\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2170 - accuracy: 0.9281 - val_loss: 0.2046 - val_accuracy: 0.9415\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2155 - accuracy: 0.9279 - val_loss: 0.2040 - val_accuracy: 0.9400\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2140 - accuracy: 0.9276 - val_loss: 0.2030 - val_accuracy: 0.9408\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2125 - accuracy: 0.9287 - val_loss: 0.2012 - val_accuracy: 0.9415\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2110 - accuracy: 0.9294 - val_loss: 0.1993 - val_accuracy: 0.9423\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2094 - accuracy: 0.9305 - val_loss: 0.1976 - val_accuracy: 0.9415\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2076 - accuracy: 0.9305 - val_loss: 0.1960 - val_accuracy: 0.9423\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2061 - accuracy: 0.9307 - val_loss: 0.1944 - val_accuracy: 0.9415\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2049 - accuracy: 0.9310 - val_loss: 0.1932 - val_accuracy: 0.9408\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2037 - accuracy: 0.9312 - val_loss: 0.1921 - val_accuracy: 0.9415\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2021 - accuracy: 0.9312 - val_loss: 0.1906 - val_accuracy: 0.9415\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2007 - accuracy: 0.9312 - val_loss: 0.1890 - val_accuracy: 0.9415\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1998 - accuracy: 0.9310 - val_loss: 0.1881 - val_accuracy: 0.9423\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1985 - accuracy: 0.9312 - val_loss: 0.1869 - val_accuracy: 0.9415\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1973 - accuracy: 0.9320 - val_loss: 0.1859 - val_accuracy: 0.9408\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1966 - accuracy: 0.9317 - val_loss: 0.1856 - val_accuracy: 0.9415\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1954 - accuracy: 0.9315 - val_loss: 0.1840 - val_accuracy: 0.9408\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1948 - accuracy: 0.9315 - val_loss: 0.1831 - val_accuracy: 0.9415\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1938 - accuracy: 0.9328 - val_loss: 0.1833 - val_accuracy: 0.9415\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1931 - accuracy: 0.9317 - val_loss: 0.1816 - val_accuracy: 0.9415\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1922 - accuracy: 0.9320 - val_loss: 0.1809 - val_accuracy: 0.9415\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1917 - accuracy: 0.9325 - val_loss: 0.1810 - val_accuracy: 0.9415\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1905 - accuracy: 0.9325 - val_loss: 0.1795 - val_accuracy: 0.9415\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1898 - accuracy: 0.9333 - val_loss: 0.1789 - val_accuracy: 0.9415\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1887 - accuracy: 0.9328 - val_loss: 0.1786 - val_accuracy: 0.9423\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1880 - accuracy: 0.9333 - val_loss: 0.1773 - val_accuracy: 0.9423\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1872 - accuracy: 0.9330 - val_loss: 0.1768 - val_accuracy: 0.9423\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1867 - accuracy: 0.9335 - val_loss: 0.1760 - val_accuracy: 0.9423\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1858 - accuracy: 0.9330 - val_loss: 0.1762 - val_accuracy: 0.9431\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1845 - accuracy: 0.9335 - val_loss: 0.1747 - val_accuracy: 0.9431\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1838 - accuracy: 0.9338 - val_loss: 0.1743 - val_accuracy: 0.9431\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1829 - accuracy: 0.9341 - val_loss: 0.1741 - val_accuracy: 0.9438\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1821 - accuracy: 0.9341 - val_loss: 0.1727 - val_accuracy: 0.9438\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1813 - accuracy: 0.9346 - val_loss: 0.1725 - val_accuracy: 0.9438\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1804 - accuracy: 0.9348 - val_loss: 0.1715 - val_accuracy: 0.9438\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1797 - accuracy: 0.9356 - val_loss: 0.1713 - val_accuracy: 0.9438\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1786 - accuracy: 0.9358 - val_loss: 0.1701 - val_accuracy: 0.9438\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1779 - accuracy: 0.9361 - val_loss: 0.1695 - val_accuracy: 0.9438\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1772 - accuracy: 0.9361 - val_loss: 0.1689 - val_accuracy: 0.9446\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1762 - accuracy: 0.9366 - val_loss: 0.1684 - val_accuracy: 0.9438\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1752 - accuracy: 0.9366 - val_loss: 0.1673 - val_accuracy: 0.9438\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1743 - accuracy: 0.9369 - val_loss: 0.1669 - val_accuracy: 0.9438\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1733 - accuracy: 0.9371 - val_loss: 0.1658 - val_accuracy: 0.9438\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1724 - accuracy: 0.9379 - val_loss: 0.1653 - val_accuracy: 0.9454\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1722 - accuracy: 0.9366 - val_loss: 0.1642 - val_accuracy: 0.9446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1709 - accuracy: 0.9379 - val_loss: 0.1646 - val_accuracy: 0.9446\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1699 - accuracy: 0.9374 - val_loss: 0.1627 - val_accuracy: 0.9446\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1689 - accuracy: 0.9384 - val_loss: 0.1630 - val_accuracy: 0.9446\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1679 - accuracy: 0.9394 - val_loss: 0.1617 - val_accuracy: 0.9446\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1669 - accuracy: 0.9392 - val_loss: 0.1608 - val_accuracy: 0.9446\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1660 - accuracy: 0.9397 - val_loss: 0.1602 - val_accuracy: 0.9446\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1657 - accuracy: 0.9389 - val_loss: 0.1595 - val_accuracy: 0.9454\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1643 - accuracy: 0.9410 - val_loss: 0.1593 - val_accuracy: 0.9454\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1633 - accuracy: 0.9402 - val_loss: 0.1576 - val_accuracy: 0.9446\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1621 - accuracy: 0.9402 - val_loss: 0.1578 - val_accuracy: 0.9462\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1617 - accuracy: 0.9423 - val_loss: 0.1564 - val_accuracy: 0.9462\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1612 - accuracy: 0.9397 - val_loss: 0.1555 - val_accuracy: 0.9462\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1610 - accuracy: 0.9400 - val_loss: 0.1554 - val_accuracy: 0.9462\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1580 - accuracy: 0.9415 - val_loss: 0.1538 - val_accuracy: 0.9469\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1576 - accuracy: 0.9430 - val_loss: 0.1542 - val_accuracy: 0.9485\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1566 - accuracy: 0.9425 - val_loss: 0.1528 - val_accuracy: 0.9485\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1560 - accuracy: 0.9423 - val_loss: 0.1517 - val_accuracy: 0.9485\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1542 - accuracy: 0.9428 - val_loss: 0.1507 - val_accuracy: 0.9477\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1526 - accuracy: 0.9425 - val_loss: 0.1510 - val_accuracy: 0.9500\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1520 - accuracy: 0.9430 - val_loss: 0.1488 - val_accuracy: 0.9485\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1507 - accuracy: 0.9433 - val_loss: 0.1492 - val_accuracy: 0.9500\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1494 - accuracy: 0.9446 - val_loss: 0.1471 - val_accuracy: 0.9492\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1487 - accuracy: 0.9441 - val_loss: 0.1462 - val_accuracy: 0.9492\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1472 - accuracy: 0.9441 - val_loss: 0.1450 - val_accuracy: 0.9485\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1456 - accuracy: 0.9453 - val_loss: 0.1436 - val_accuracy: 0.9485\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1442 - accuracy: 0.9448 - val_loss: 0.1435 - val_accuracy: 0.9485\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1424 - accuracy: 0.9471 - val_loss: 0.1407 - val_accuracy: 0.9477\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1404 - accuracy: 0.9459 - val_loss: 0.1396 - val_accuracy: 0.9492\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1365 - accuracy: 0.9477 - val_loss: 0.1373 - val_accuracy: 0.9523\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9497 - val_loss: 0.1315 - val_accuracy: 0.9523\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1255 - accuracy: 0.9505 - val_loss: 0.1283 - val_accuracy: 0.9523\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1233 - accuracy: 0.9510 - val_loss: 0.1269 - val_accuracy: 0.9538\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1198 - accuracy: 0.9536 - val_loss: 0.1233 - val_accuracy: 0.9538\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1174 - accuracy: 0.9538 - val_loss: 0.1210 - val_accuracy: 0.9546\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1164 - accuracy: 0.9543 - val_loss: 0.1198 - val_accuracy: 0.9546\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1154 - accuracy: 0.9559 - val_loss: 0.1177 - val_accuracy: 0.9546\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1141 - accuracy: 0.9569 - val_loss: 0.1182 - val_accuracy: 0.9592\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1113 - accuracy: 0.9584 - val_loss: 0.1152 - val_accuracy: 0.9562\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1108 - accuracy: 0.9577 - val_loss: 0.1158 - val_accuracy: 0.9538\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1113 - accuracy: 0.9564 - val_loss: 0.1207 - val_accuracy: 0.9531\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1122 - accuracy: 0.9584 - val_loss: 0.1173 - val_accuracy: 0.9538\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1116 - accuracy: 0.9589 - val_loss: 0.1111 - val_accuracy: 0.9600\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1073 - accuracy: 0.9587 - val_loss: 0.1105 - val_accuracy: 0.9600\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1050 - accuracy: 0.9613 - val_loss: 0.1105 - val_accuracy: 0.9638\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1043 - accuracy: 0.9615 - val_loss: 0.1105 - val_accuracy: 0.9662\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1023 - accuracy: 0.9633 - val_loss: 0.1074 - val_accuracy: 0.9646\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1014 - accuracy: 0.9633 - val_loss: 0.1127 - val_accuracy: 0.9677\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1048 - accuracy: 0.9613 - val_loss: 0.1100 - val_accuracy: 0.9685\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1015 - accuracy: 0.9641 - val_loss: 0.1047 - val_accuracy: 0.9638\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1016 - accuracy: 0.9630 - val_loss: 0.1053 - val_accuracy: 0.9623\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1003 - accuracy: 0.9646 - val_loss: 0.1037 - val_accuracy: 0.9677\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1007 - accuracy: 0.9633 - val_loss: 0.1056 - val_accuracy: 0.9615\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0967 - accuracy: 0.9679 - val_loss: 0.1019 - val_accuracy: 0.9638\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0973 - accuracy: 0.9672 - val_loss: 0.1021 - val_accuracy: 0.9631\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0980 - accuracy: 0.9630 - val_loss: 0.1018 - val_accuracy: 0.9692\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0950 - accuracy: 0.9656 - val_loss: 0.1009 - val_accuracy: 0.9692\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0941 - accuracy: 0.9659 - val_loss: 0.1010 - val_accuracy: 0.9708\n",
      "Epoch 116/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0953 - accuracy: 0.9677 - val_loss: 0.1059 - val_accuracy: 0.9723\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0944 - accuracy: 0.9677 - val_loss: 0.0989 - val_accuracy: 0.9708\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0915 - accuracy: 0.9700 - val_loss: 0.0981 - val_accuracy: 0.9638\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0914 - accuracy: 0.9690 - val_loss: 0.0991 - val_accuracy: 0.9708\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0909 - accuracy: 0.9692 - val_loss: 0.0982 - val_accuracy: 0.9708\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0895 - accuracy: 0.9705 - val_loss: 0.0951 - val_accuracy: 0.9708\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0891 - accuracy: 0.9700 - val_loss: 0.0962 - val_accuracy: 0.9669\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0944 - accuracy: 0.9672 - val_loss: 0.1051 - val_accuracy: 0.9592\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0918 - accuracy: 0.9705 - val_loss: 0.0951 - val_accuracy: 0.9685\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0914 - accuracy: 0.9692 - val_loss: 0.0942 - val_accuracy: 0.9677\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0977 - accuracy: 0.9638 - val_loss: 0.0925 - val_accuracy: 0.9708\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0915 - accuracy: 0.9702 - val_loss: 0.0922 - val_accuracy: 0.9715\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0878 - accuracy: 0.9715 - val_loss: 0.0916 - val_accuracy: 0.9731\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0852 - accuracy: 0.9715 - val_loss: 0.0921 - val_accuracy: 0.9738\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0842 - accuracy: 0.9731 - val_loss: 0.0908 - val_accuracy: 0.9700\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0847 - accuracy: 0.9723 - val_loss: 0.0892 - val_accuracy: 0.9731\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0838 - accuracy: 0.9738 - val_loss: 0.0905 - val_accuracy: 0.9685\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0826 - accuracy: 0.9741 - val_loss: 0.0886 - val_accuracy: 0.9723\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0820 - accuracy: 0.9733 - val_loss: 0.0879 - val_accuracy: 0.9715\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0823 - accuracy: 0.9723 - val_loss: 0.0927 - val_accuracy: 0.9731\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0825 - accuracy: 0.9736 - val_loss: 0.0899 - val_accuracy: 0.9754\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0832 - accuracy: 0.9723 - val_loss: 0.0864 - val_accuracy: 0.9746\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0827 - accuracy: 0.9725 - val_loss: 0.0863 - val_accuracy: 0.9738\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0838 - accuracy: 0.9715 - val_loss: 0.0863 - val_accuracy: 0.9738\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0785 - accuracy: 0.9761 - val_loss: 0.0865 - val_accuracy: 0.9746\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0807 - accuracy: 0.9749 - val_loss: 0.0865 - val_accuracy: 0.9762\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0804 - accuracy: 0.9728 - val_loss: 0.0871 - val_accuracy: 0.9769\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0786 - accuracy: 0.9741 - val_loss: 0.0866 - val_accuracy: 0.9762\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0778 - accuracy: 0.9759 - val_loss: 0.0844 - val_accuracy: 0.9762\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0772 - accuracy: 0.9756 - val_loss: 0.0836 - val_accuracy: 0.9769\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0760 - accuracy: 0.9766 - val_loss: 0.0820 - val_accuracy: 0.9777\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0757 - accuracy: 0.9777 - val_loss: 0.0817 - val_accuracy: 0.9777\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0753 - accuracy: 0.9764 - val_loss: 0.0829 - val_accuracy: 0.9762\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0757 - accuracy: 0.9754 - val_loss: 0.0885 - val_accuracy: 0.9769\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0771 - accuracy: 0.9749 - val_loss: 0.0812 - val_accuracy: 0.9777\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0739 - accuracy: 0.9772 - val_loss: 0.0806 - val_accuracy: 0.9785\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0740 - accuracy: 0.9782 - val_loss: 0.0804 - val_accuracy: 0.9777\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0733 - accuracy: 0.9777 - val_loss: 0.0799 - val_accuracy: 0.9769\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0726 - accuracy: 0.9774 - val_loss: 0.0817 - val_accuracy: 0.9762\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0730 - accuracy: 0.9779 - val_loss: 0.0788 - val_accuracy: 0.9785\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0722 - accuracy: 0.9790 - val_loss: 0.0780 - val_accuracy: 0.9785\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0709 - accuracy: 0.9784 - val_loss: 0.0790 - val_accuracy: 0.9785\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0707 - accuracy: 0.9784 - val_loss: 0.0778 - val_accuracy: 0.9777\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0716 - accuracy: 0.9787 - val_loss: 0.0769 - val_accuracy: 0.9785\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0702 - accuracy: 0.9797 - val_loss: 0.0769 - val_accuracy: 0.9785\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0694 - accuracy: 0.9792 - val_loss: 0.0767 - val_accuracy: 0.9777\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0699 - accuracy: 0.9784 - val_loss: 0.0761 - val_accuracy: 0.9792\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0682 - accuracy: 0.9792 - val_loss: 0.0779 - val_accuracy: 0.9800\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0693 - accuracy: 0.9790 - val_loss: 0.0751 - val_accuracy: 0.9792\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0689 - accuracy: 0.9787 - val_loss: 0.0758 - val_accuracy: 0.9792\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0691 - accuracy: 0.9777 - val_loss: 0.0744 - val_accuracy: 0.9792\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0689 - accuracy: 0.9792 - val_loss: 0.0786 - val_accuracy: 0.9769\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0704 - accuracy: 0.9769 - val_loss: 0.0774 - val_accuracy: 0.9777\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0680 - accuracy: 0.9782 - val_loss: 0.0734 - val_accuracy: 0.9800\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0666 - accuracy: 0.9805 - val_loss: 0.0743 - val_accuracy: 0.9792\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0667 - accuracy: 0.9795 - val_loss: 0.0731 - val_accuracy: 0.9800\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0665 - accuracy: 0.9805 - val_loss: 0.0747 - val_accuracy: 0.9800\n",
      "Epoch 173/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0709 - accuracy: 0.9774 - val_loss: 0.0729 - val_accuracy: 0.9800\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0729 - accuracy: 0.9749 - val_loss: 0.0739 - val_accuracy: 0.9800\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 0.0718 - val_accuracy: 0.9808\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0653 - accuracy: 0.9802 - val_loss: 0.0723 - val_accuracy: 0.9815\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0646 - accuracy: 0.9805 - val_loss: 0.0722 - val_accuracy: 0.9808\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0640 - accuracy: 0.9808 - val_loss: 0.0767 - val_accuracy: 0.9808\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0668 - accuracy: 0.9790 - val_loss: 0.0713 - val_accuracy: 0.9815\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0664 - accuracy: 0.9792 - val_loss: 0.0722 - val_accuracy: 0.9815\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9813 - val_loss: 0.0715 - val_accuracy: 0.9808\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0630 - accuracy: 0.9805 - val_loss: 0.0704 - val_accuracy: 0.9815\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0626 - accuracy: 0.9823 - val_loss: 0.0703 - val_accuracy: 0.9815\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.9813 - val_loss: 0.0715 - val_accuracy: 0.9823\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0615 - accuracy: 0.9820 - val_loss: 0.0700 - val_accuracy: 0.9808\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0620 - accuracy: 0.9818 - val_loss: 0.0696 - val_accuracy: 0.9815\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0622 - accuracy: 0.9810 - val_loss: 0.0691 - val_accuracy: 0.9823\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0619 - accuracy: 0.9815 - val_loss: 0.0703 - val_accuracy: 0.9838\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0612 - accuracy: 0.9823 - val_loss: 0.0686 - val_accuracy: 0.9815\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0615 - accuracy: 0.9828 - val_loss: 0.0704 - val_accuracy: 0.9838\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0637 - accuracy: 0.9828 - val_loss: 0.0701 - val_accuracy: 0.9838\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0610 - accuracy: 0.9820 - val_loss: 0.0680 - val_accuracy: 0.9823\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.0694 - val_accuracy: 0.9800\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9828 - val_loss: 0.0712 - val_accuracy: 0.9800\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0629 - accuracy: 0.9802 - val_loss: 0.0689 - val_accuracy: 0.9823\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0608 - accuracy: 0.9823 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0591 - accuracy: 0.9831 - val_loss: 0.0676 - val_accuracy: 0.9846\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9831 - val_loss: 0.0669 - val_accuracy: 0.9838\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9828 - val_loss: 0.0678 - val_accuracy: 0.9823\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0601 - accuracy: 0.9820 - val_loss: 0.0714 - val_accuracy: 0.9785\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0620 - accuracy: 0.9815 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0586 - accuracy: 0.9828 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0582 - accuracy: 0.9838 - val_loss: 0.0661 - val_accuracy: 0.9838\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0582 - accuracy: 0.9831 - val_loss: 0.0652 - val_accuracy: 0.9838\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9820 - val_loss: 0.0668 - val_accuracy: 0.9854\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0677 - val_accuracy: 0.9815\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0590 - accuracy: 0.9823 - val_loss: 0.0645 - val_accuracy: 0.9838\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0577 - accuracy: 0.9838 - val_loss: 0.0642 - val_accuracy: 0.9838\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.0696 - val_accuracy: 0.9831\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0597 - accuracy: 0.9820 - val_loss: 0.0665 - val_accuracy: 0.9854\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0609 - accuracy: 0.9831 - val_loss: 0.0645 - val_accuracy: 0.9854\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0568 - accuracy: 0.9854 - val_loss: 0.0632 - val_accuracy: 0.9838\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.9818 - val_loss: 0.0632 - val_accuracy: 0.9846\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0573 - accuracy: 0.9836 - val_loss: 0.0630 - val_accuracy: 0.9838\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0561 - accuracy: 0.9836 - val_loss: 0.0636 - val_accuracy: 0.9854\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0587 - accuracy: 0.9820 - val_loss: 0.0644 - val_accuracy: 0.9831\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0579 - accuracy: 0.9849 - val_loss: 0.0644 - val_accuracy: 0.9831\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0570 - accuracy: 0.9828 - val_loss: 0.0635 - val_accuracy: 0.9862\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0554 - accuracy: 0.9831 - val_loss: 0.0636 - val_accuracy: 0.9862\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0564 - accuracy: 0.9836 - val_loss: 0.0667 - val_accuracy: 0.9838\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0572 - accuracy: 0.9828 - val_loss: 0.0633 - val_accuracy: 0.9838\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0586 - accuracy: 0.9828 - val_loss: 0.0630 - val_accuracy: 0.9846\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0549 - accuracy: 0.9836 - val_loss: 0.0631 - val_accuracy: 0.9846\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0562 - accuracy: 0.9843 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0552 - accuracy: 0.9818 - val_loss: 0.0712 - val_accuracy: 0.9831\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0604 - accuracy: 0.9813 - val_loss: 0.0705 - val_accuracy: 0.9823\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9826 - val_loss: 0.0623 - val_accuracy: 0.9854\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0616 - val_accuracy: 0.9862\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0537 - accuracy: 0.9843 - val_loss: 0.0614 - val_accuracy: 0.9862\n",
      "Epoch 230/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0549 - accuracy: 0.9843 - val_loss: 0.0615 - val_accuracy: 0.9846\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9856 - val_loss: 0.0613 - val_accuracy: 0.9862\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.0609 - val_accuracy: 0.9862\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0543 - accuracy: 0.9846 - val_loss: 0.0608 - val_accuracy: 0.9846\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0537 - accuracy: 0.9849 - val_loss: 0.0605 - val_accuracy: 0.9854\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0528 - accuracy: 0.9846 - val_loss: 0.0614 - val_accuracy: 0.9846\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0534 - accuracy: 0.9849 - val_loss: 0.0608 - val_accuracy: 0.9854\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0548 - accuracy: 0.9841 - val_loss: 0.0601 - val_accuracy: 0.9862\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0550 - accuracy: 0.9826 - val_loss: 0.0607 - val_accuracy: 0.9838\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.9851 - val_loss: 0.0607 - val_accuracy: 0.9838\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0542 - accuracy: 0.9851 - val_loss: 0.0617 - val_accuracy: 0.9854\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0539 - accuracy: 0.9841 - val_loss: 0.0649 - val_accuracy: 0.9854\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0551 - accuracy: 0.9856 - val_loss: 0.0677 - val_accuracy: 0.9838\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.9828 - val_loss: 0.0731 - val_accuracy: 0.9831\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0589 - accuracy: 0.9805 - val_loss: 0.0641 - val_accuracy: 0.9854\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0592 - accuracy: 0.9818 - val_loss: 0.0600 - val_accuracy: 0.9846\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0535 - accuracy: 0.9851 - val_loss: 0.0601 - val_accuracy: 0.9854\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0526 - accuracy: 0.9859 - val_loss: 0.0610 - val_accuracy: 0.9854\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0520 - accuracy: 0.9846 - val_loss: 0.0596 - val_accuracy: 0.9846\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0521 - accuracy: 0.9854 - val_loss: 0.0614 - val_accuracy: 0.9854\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0540 - accuracy: 0.9849 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.0602 - val_accuracy: 0.9862\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0519 - accuracy: 0.9849 - val_loss: 0.0628 - val_accuracy: 0.9846\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0530 - accuracy: 0.9849 - val_loss: 0.0594 - val_accuracy: 0.9854\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0517 - accuracy: 0.9864 - val_loss: 0.0627 - val_accuracy: 0.9831\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0571 - accuracy: 0.9836 - val_loss: 0.0598 - val_accuracy: 0.9854\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0672 - val_accuracy: 0.9846\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9843 - val_loss: 0.0601 - val_accuracy: 0.9854\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0507 - accuracy: 0.9867 - val_loss: 0.0591 - val_accuracy: 0.9854\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 0.0595 - val_accuracy: 0.9846\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9867 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0511 - accuracy: 0.9861 - val_loss: 0.0619 - val_accuracy: 0.9862\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0520 - accuracy: 0.9869 - val_loss: 0.0640 - val_accuracy: 0.9862\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9854 - val_loss: 0.0591 - val_accuracy: 0.9854\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0502 - accuracy: 0.9864 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0498 - accuracy: 0.9864 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0503 - accuracy: 0.9859 - val_loss: 0.0583 - val_accuracy: 0.9854\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0497 - accuracy: 0.9864 - val_loss: 0.0599 - val_accuracy: 0.9862\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.0593 - val_accuracy: 0.9854\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0501 - accuracy: 0.9869 - val_loss: 0.0614 - val_accuracy: 0.9862\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9851 - val_loss: 0.0607 - val_accuracy: 0.9862\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9854 - val_loss: 0.0590 - val_accuracy: 0.9869\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0523 - accuracy: 0.9861 - val_loss: 0.0589 - val_accuracy: 0.9869\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0502 - accuracy: 0.9869 - val_loss: 0.0612 - val_accuracy: 0.9862\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9859 - val_loss: 0.0594 - val_accuracy: 0.9854\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9854 - val_loss: 0.0591 - val_accuracy: 0.9869\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0508 - accuracy: 0.9849 - val_loss: 0.0605 - val_accuracy: 0.9869\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0489 - accuracy: 0.9872 - val_loss: 0.0585 - val_accuracy: 0.9862\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0495 - accuracy: 0.9861 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0493 - accuracy: 0.9851 - val_loss: 0.0591 - val_accuracy: 0.9869\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9869 - val_loss: 0.0590 - val_accuracy: 0.9869\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9877 - val_loss: 0.0588 - val_accuracy: 0.9869\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9872 - val_loss: 0.0590 - val_accuracy: 0.9862\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9869 - val_loss: 0.0585 - val_accuracy: 0.9862\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0491 - accuracy: 0.9864 - val_loss: 0.0586 - val_accuracy: 0.9877\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9851 - val_loss: 0.0622 - val_accuracy: 0.9869\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0608 - val_accuracy: 0.9862\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9715\n",
      "Test accuracy: 0.9715384840965271\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
